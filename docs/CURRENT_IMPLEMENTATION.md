# Current Scraper Implementation

## Overview
The existing scraper targets the Cayman Islands Judicial website’s unreported judgments page. It pulls the published judgments CSV on each run, interprets the `Actions` column tokens, and drives a Playwright-based flow (with legacy Selenium helpers unused in the current path) that issues the official `dl_bfile` AJAX requests to Box.com. Downloads, metadata, and resume information remain stored on disk using JSON/CSV files; SQLite now tracks CSV versions, runs, cases, and per-case download attempts and is read from for reporting (`/api/runs/latest`, `/api/db/...`, and optionally `/report` and `/api/downloaded-cases`). Scraper control flow still uses JSON/state for checkpoints and resume. DB-backed reporting helpers power the `/api/db/...` endpoints and, when `BAILIIKC_USE_DB_REPORTING=1`, also feed `/report` and `/api/downloaded-cases` with only downloaded cases while keeping the JSON row shape intact. Legacy JSON files remain the default source for reporting when the flag is not set.

## Key Modules
- **app/main.py**: Flask web UI with forms for scrape/resume/reset actions, webhook endpoint, and routes for reports, exports, and file serving. Launches background scrape threads that call `run_scrape`. `/report` and `/api/downloaded-cases` read JSON logs by default but switch to SQLite (still only downloaded rows) when `BAILIIKC_USE_DB_REPORTING=1`. `/api/runs/latest` is DB-backed via `db_reporting` for run metadata and download aggregates. The app also exposes `/api/db/runs/latest`, `/api/db/downloaded-cases`, and `/api/db/runs/<run_id>/downloaded-cases` endpoints backed by SQLite.
- **main.py (repo root)**: Thin entrypoint that imports `app.main` (which initialises directories and schema) and starts the Flask app; suitable for local development or generic hosting environments.
- **app/scraper/config.py**: Central constants for data paths, URLs, defaults, and HTTP headers. Defines `/app/data` layout, scrape defaults, and helper predicates for mode detection.
- **app/scraper/run.py**: Primary scraper engine using Playwright. Loads the judgments CSV, builds in-memory case indices, coordinates page navigation and AJAX monitoring, downloads PDFs, and writes metadata/logs/state. Contains checkpoint logic and resume handling.
- **app/scraper/cases_index.py**: CSV loader and normaliser. Parses `Actions` tokens, builds `CASES_BY_ACTION`, `AJAX_FNAME_INDEX`, and `CASES_ALL` for lookup during scraping.
- **app/scraper/downloader.py**: Legacy Selenium-based downloader helpers (currently unused by Playwright flow). Handles AJAX nonce acquisition, Box URL fetching, PDF streaming, duplicate detection, and metadata updates.
- **app/scraper/selenium_client.py**: Selenium utilities for nonce extraction and AJAX POST execution to retrieve Box URLs. Used by the legacy downloader path.
- **app/scraper/utils.py**: Shared utilities: directory setup, logging configuration, filename sanitisation, PDF path building, metadata persistence, ZIP generation, JSON helpers, and duplicate detection.
- **app/scraper/state.py**: Checkpoint persistence and derivation from logs (`state.json`, scrape log parsing) to support resume-on-crash flows.
- **app/scraper/telemetry.py**: Lightweight telemetry writer producing per-run JSON files for Excel export plus pruning utilities for generated workbooks.
- **app/scraper/export_excel.py**: Builds Excel workbooks from telemetry JSON for download via the API.
- **app/scraper/db_reporting.py**: Read-only helpers for querying runs and downloads from SQLite to support DB-backed reporting.
- **app/scraper/worklist.py**: DB-backed helpers for assembling per-run worklists (lists of cases to process) from the SQLite ``cases`` table for a given ``csv_version_id``. Supports ``build_full_worklist`` and ``build_new_worklist`` plus a dispatcher ``build_worklist(...)``; ``resume`` is stubbed as ``NotImplementedError``. The scraper now uses these helpers for ``mode="new"`` and ``mode="full"`` when the corresponding flags are enabled.

## Current Scrape Workflow
1. **UI submission**: `app/main.py` renders forms and reads user input (base URL, waits, limits, resume options). On submit, it saves defaults, optionally resets state, and starts a background thread that calls `run_scrape` with the collected parameters.
2. **CSV load and case index**: `run.py` syncs `judgments.csv` via `csv_sync.sync_csv`, recording a `csv_versions` row plus a concrete CSV file path and row count. That path is then passed into `load_cases_index` so the in-memory indices (`CASES_BY_ACTION`, etc.) are built from the exact payload tied to the run. When `BAILIIKC_USE_DB_CASES=1`, the CSV path is still recorded for observability while the index is built from SQLite instead.
3. **Playwright session**: The scraper launches Chromium, loads the target page, scrolls to trigger DataTables loading, and navigates through pages/rows. It monitors `admin-ajax.php` responses to capture `dl_bfile` payloads and Box URLs.
4. **Download handling**: When a Box URL is observed, `handle_dl_bfile_from_ajax` streams the PDF (via Playwright’s `context.request`), writes files under `/app/data/pdfs`, updates in-memory metadata, and appends entries to `downloads.jsonl`. Filename fallbacks and duplicate checks rely on helpers from `utils.py`.
5. **Resume/state**: `Checkpoint` objects inside `run.py`, plus `state.py` helpers and scrape logs, maintain progress (`state.json`, `run_state.json`, latest scrape log). Resume modes decide whether to reuse these checkpoints or restart.
6. **Reporting**: After the run, summaries/log paths are stored in JSON files. The Flask report page reads `downloads.jsonl`, `last_summary.json`, and current logs to show tables, filters, and download links by default. When `BAILIIKC_USE_DB_REPORTING=1`, `/report` and `/api/downloaded-cases` instead read downloaded rows from SQLite via `db_reporting` but return the same row structure (`actions_token`, `title`, `subject`, `court`, `category`, `judgment_date`, `sort_judgment_date`, `cause_number`, `downloaded_at`, `saved_path`, `filename`, `size_kb`). Telemetry is also written to per-run JSON for Excel export.

## Current Data & State Files
- **metadata.json**: Primary metadata store with `downloads` list; updated on each successful download.
- **downloads.jsonl**: Append-only log of download attempts with actions token, titles, sizes, timestamps, and saved paths (used for the report table).
- **state.json**: Persisted checkpoint referenced by resume logic.
- **run_state.json**: Additional run progress tracking (written by `save_checkpoint`).
- **last_summary.json**: Summary of the most recent run (counts, mode, log file path) for display in the UI.
- **scrape_log.txt / scrape_*.log**: Human-readable scrape logs stored under `/app/data/logs`, tailed by the UI for live updates.

## SQLite usage (logging by default)
- **CSV sync**: Each run syncs `judgments.csv` via `csv_sync.sync_csv`, recording a `csv_versions` row and upserting `cases`. `first_seen_version_id` and `last_seen_version_id` encode when each case first and last appears, while `is_active` marks removals within the feed.
- **Runs table**: `run_scrape` inserts a row into `runs` for each scrape attempt. The `trigger` column records the entrypoint (`"ui"` for web UI runs, `"webhook"` for ChangeDetection.io webhook runs, `"cli"` for direct programmatic invocations), while `mode` captures the effective scrape mode (`"full"`, `"new"`, or `"resume"`). Completion and failures are marked at the end of the run. `db_reporting.get_latest_run_id` and `get_run_summary` provide read-only access for reporting APIs.
- **Run list (DB-backed)**: `db_reporting.list_recent_runs(limit)` reads from the `runs` table and returns the most recent rows ordered by `started_at` DESC. `GET /api/db/runs` exposes this as JSON with `{ok, count, runs}`, where each run entry includes `id`, `trigger`, `mode`, `csv_version_id`, `status`, `started_at`, `ended_at`, and `error_summary`. An optional `?limit=` query parameter controls how many rows are returned (bounded server-side).
- **Downloads table**: Per-case attempts are logged during scraping with statuses (`pending`, `in_progress`, `downloaded`, `skipped`, `failed`), attempt counts, timestamps, optional file info, and error details. `/api/db/downloaded-cases` reads from this table to offer DataTables-compatible payloads without touching the legacy JSON files, `/api/db/runs/<run_id>/downloaded-cases` exposes the successful rows for a specific run (powered by `db_reporting.get_downloaded_cases_for_run`), and `/report` plus `/api/downloaded-cases` can switch to this data (still filtered to downloaded cases) when `BAILIIKC_USE_DB_REPORTING=1`.
- **Downloaded cases per run (DB-backed)**: `db_reporting.get_downloaded_cases_for_run(run_id)` joins `downloads` and `cases` to return the successful rows for the given `run_id` as dictionaries. `GET /api/db/runs/<run_id>/downloaded-cases` returns `{ok: true, run_id, count, downloads}` (with `<run_id>` as the path parameter) and responds with 404 when the run does not exist.
- **CSV version case diff (DB-backed)**: `db_reporting.get_case_diff_for_csv_version(version_id)` derives which cases are new at a version (`first_seen_version_id == version_id`) and which were removed at that version (`last_seen_version_id == version_id` and `is_active = 0`) for `source = 'unreported_judgments'`. `GET /api/db/csv_versions/<version_id>/case-diff` returns `{ok: true, csv_version_id, new_count, removed_count, new_cases, removed_cases}` (with `<version_id>` as the path parameter) and responds with 404 when the version does not exist or is invalid.
- **Case index backend**: The scraper still builds its in-memory case index from the CSV by default. Setting `BAILIIKC_USE_DB_CASES=1` makes `cases_index` load from the SQLite `cases` table instead; this mode is intended for validation and should be behaviourally identical to the CSV path.
- **BAILIIKC_USE_DB_WORKLIST_FOR_NEW**: when set to `"1"`, `scrape_mode="new"` uses the DB-backed worklist from `app.scraper.worklist.build_new_worklist(csv_version_id, source)` derived from the SQLite `cases` table. When unset, new-only mode relies on the legacy CSV-driven planner.
- **BAILIIKC_USE_DB_WORKLIST_FOR_FULL**: when set to `"1"`, `scrape_mode="full"` uses the DB-backed worklist from `build_full_worklist(...)`. When unset, full mode continues to use the legacy CSV-only path.
- **DB worklist helpers**: `app.scraper.worklist.build_full_worklist(csv_version_id, source)` and `build_new_worklist(...)` derive the set of active, non-criminal cases to process for a given CSV version from the SQLite `cases` table. A dispatcher `build_worklist(mode, csv_version_id, source)` chooses the appropriate helper for `"full"` and `"new"` modes and raises `NotImplementedError` for `"resume"`. When `BAILIIKC_USE_DB_WORKLIST_FOR_NEW="1"` or `BAILIIKC_USE_DB_WORKLIST_FOR_FULL="1"` are set, `run.py` uses these helpers to plan the cases for `mode="new"` or `mode="full"`; legacy CSV-driven planning remains the default when flags are off.
- **Behaviour**: JSON files remain the source of truth for scraper control/resume. SQLite is read for reporting (/api/runs/latest, /api/db/ endpoints, and optional DB-backed /report or /api/downloaded-cases) while continuing to mirror run and download activity during a scrape.
- **Consistency checker (JSON vs DB)**: `app.scraper.consistency.compare_latest_downloads_json_vs_db()` computes a diagnostic report comparing the JSON-based and DB-based views of downloaded cases for the latest run. This is exposed as a CLI via `python -m app.scraper.consistency` and is intended for internal validation while DB-backed reporting and control flow are adopted. Any errors or mismatches cause the CLI to exit non-zero.

## Known Limitations / Fragility
- The judgments CSV is still fetched from the remote URL on each run. While `csv_sync.sync_csv` now records `csv_versions` rows and local CSV files for versioning, there is no offline fallback when the source is unavailable, so network hiccups can still prevent a run from starting.
- Resume relies on JSON checkpoints and log parsing, leading to complexity when recovering mid-run.
- Disk space guardrails are basic (free-space checks via `utils.disk_has_room`), and filename length issues require fallbacks.
- The Playwright AJAX capture flow is delicate; nonce/session handling and Box URL extraction were tuned through trial and error and should remain untouched for now.
